# EntraÃ®nement amÃ©liorÃ© avec gestion des sauvegardes multiples
# Compatible Python 3.13.2

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
import os

# Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ Utilisation de : {device}")

# CrÃ©er un dossier pour les modÃ¨les
if not os.path.exists('modeles'):
    os.makedirs('modeles')

print("ğŸ¤– EntraÃ®nement amÃ©liorÃ© de l'IA")
print("=" * 60)

# 1. CONFIGURATION DE L'ENTRAÃNEMENT
print("âš™ï¸ Configuration de l'entraÃ®nement...")

# ParamÃ¨tres modifiables
EPOCHS = 20  # Plus d'Ã©poques = meilleur apprentissage
BATCH_SIZE = 32
LEARNING_RATE = 0.001
DROPOUT_RATE = 0.8

print(f"   ğŸ“Š Ã‰poques : {EPOCHS}")
print(f"   ğŸ“¦ Taille des lots : {BATCH_SIZE}")
print(f"   ğŸ¯ Taux d'apprentissage : {LEARNING_RATE}")
print(f"   ğŸ›¡ï¸ Dropout : {DROPOUT_RATE}")

# 2. CHARGEMENT DES DONNÃ‰ES AVEC AUGMENTATION
print("\nğŸ“š Chargement des donnÃ©es avec augmentation...")

# Transformations pour l'entraÃ®nement (avec augmentation)
transform_train = transforms.Compose([
    transforms.RandomRotation(10),  # Rotation alÃ©atoire
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Translation
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Transformations pour le test (sans augmentation)
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Datasets
train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform_train)
test_dataset = datasets.MNIST('data', train=False, transform=transform_test)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

print(f"âœ… DonnÃ©es chargÃ©es avec augmentation !")

# 3. RÃ‰SEAU AMÃ‰LIORÃ‰
print("\nğŸ§  Construction du rÃ©seau amÃ©liorÃ©...")

class ReseauChiffresAmeliore(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super(ReseauChiffresAmeliore, self).__init__()
        
        # Plus de couches pour plus de puissance
        self.fc1 = nn.Linear(28*28, 256)  # Plus de neurones
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 10)
        
        # Normalisation par lots
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(64)
        
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        x = x.view(-1, 28*28)
        
        # Couche 1
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        
        # Couche 2
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        
        # Couche 3
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout(x)
        
        # Couche 4
        x = F.relu(self.fc4(x))
        x = self.dropout(x)
        
        # Couche de sortie
        x = F.log_softmax(self.fc5(x), dim=1)
        return x

# CrÃ©er le modÃ¨le
model = ReseauChiffresAmeliore(dropout_rate=DROPOUT_RATE).to(device)
total_params = sum(p.numel() for p in model.parameters())
print(f"âœ… RÃ©seau amÃ©liorÃ© crÃ©Ã© ! ParamÃ¨tres : {total_params:,}")

# 4. CONFIGURATION AVANCÃ‰E
print("\nâš™ï¸ Configuration avancÃ©e...")

# Optimiseur avec scheduler
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)
criterion = nn.NLLLoss()

# Variables pour tracking
train_losses = []
train_accuracies = []
test_accuracies = []
best_accuracy = 0
best_model_path = ""

# 5. FONCTIONS D'ENTRAÃNEMENT AMÃ‰LIORÃ‰ES
def entrainer_une_epoque(epoch):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        # Affichage moins frÃ©quent
        if batch_idx % 300 == 0:
            print(f'   Ã‰poque {epoch}, Batch {batch_idx:3d}/{len(train_loader)} - '
                  f'Perte: {loss.item():.4f}, PrÃ©cision: {100. * correct / total:.1f}%')
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def tester():
    model.eval()
    test_loss = 0
    correct = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    accuracy = 100. * correct / len(test_dataset)
    return accuracy

# 6. ENTRAÃNEMENT PRINCIPAL
print(f"\nğŸ‹ï¸ EntraÃ®nement sur {EPOCHS} Ã©poques...")
print("   (Plus d'Ã©poques = meilleure performance)")

# Timestamp pour les noms de fichiers
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

for epoch in range(1, EPOCHS + 1):
    print(f"\n--- Ã‰poque {epoch}/{EPOCHS} ---")
    
    # EntraÃ®nement
    train_loss, train_acc = entrainer_une_epoque(epoch)
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    
    # Test
    test_acc = tester()
    test_accuracies.append(test_acc)
    
    # Mise Ã  jour du learning rate
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']
    
    print(f"âœ… Ã‰poque {epoch} terminÃ©e:")
    print(f"   - PrÃ©cision entraÃ®nement: {train_acc:.2f}%")
    print(f"   - PrÃ©cision test: {test_acc:.2f}%")
    print(f"   - Learning rate: {current_lr:.6f}")
    
    # Sauvegarder le meilleur modÃ¨le
    if test_acc > best_accuracy:
        best_accuracy = test_acc
        best_model_path = f'modeles/meilleur_modele_{timestamp}_{test_acc:.1f}pct.pth'
        torch.save(model.state_dict(), best_model_path)
        print(f"   ğŸ† Nouveau record ! SauvegardÃ© : {best_model_path}")
    
    # Sauvegarder aussi Ã  chaque Ã©poque
    epoch_path = f'modeles/modele_epoque_{epoch}_{timestamp}_{test_acc:.1f}pct.pth'
    torch.save(model.state_dict(), epoch_path)

# 7. SAUVEGARDE FINALE
final_path = f'modeles/modele_final_{timestamp}_{test_accuracies[-1]:.1f}pct.pth'
torch.save(model.state_dict(), final_path)

# Aussi sauvegarder comme fichier principal (pour compatibilitÃ©)
torch.save(model.state_dict(), 'modele_chiffres_pytorch.pth')

print(f"\nğŸ‰ EntraÃ®nement terminÃ© !")
print(f"ğŸ¯ PrÃ©cision finale : {test_accuracies[-1]:.2f}%")
print(f"ğŸ† Meilleure prÃ©cision : {best_accuracy:.2f}%")

# 8. GRAPHIQUES DÃ‰TAILLÃ‰S
print("\nğŸ“ˆ CrÃ©ation des graphiques...")
plt.figure(figsize=(18, 6))

# PrÃ©cision
plt.subplot(1, 3, 1)
epochs_range = range(1, EPOCHS + 1)
plt.plot(epochs_range, train_accuracies, 'bo-', label='EntraÃ®nement', linewidth=2)
plt.plot(epochs_range, test_accuracies, 'ro-', label='Test', linewidth=2)
plt.title('Ã‰volution de la prÃ©cision', fontsize=14)
plt.xlabel('Ã‰poque')
plt.ylabel('PrÃ©cision (%)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(0, 100)

# Perte
plt.subplot(1, 3, 2)
plt.plot(epochs_range, train_losses, 'go-', linewidth=2)
plt.title('Ã‰volution de la perte', fontsize=14)
plt.xlabel('Ã‰poque')
plt.ylabel('Perte')
plt.grid(True, alpha=0.3)

# Comparaison finale
plt.subplot(1, 3, 3)
plt.bar(['EntraÃ®nement', 'Test'], [train_accuracies[-1], test_accuracies[-1]], 
        color=['blue', 'red'], alpha=0.7)
plt.title('PrÃ©cision finale', fontsize=14)
plt.ylabel('PrÃ©cision (%)')
plt.ylim(0, 100)

# Ajouter les valeurs sur les barres
for i, v in enumerate([train_accuracies[-1], test_accuracies[-1]]):
    plt.text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

# 9. RÃ‰SUMÃ‰ DES FICHIERS SAUVEGARDÃ‰S
print("\nğŸ“ Fichiers sauvegardÃ©s :")
print(f"   ğŸ† Meilleur modÃ¨le : {best_model_path}")
print(f"   ğŸ“Š ModÃ¨le final : {final_path}")
print(f"   ğŸ”„ ModÃ¨le principal : modele_chiffres_pytorch.pth")
print(f"   ğŸ“‚ Dossier : modeles/")

# 10. CONSEILS POUR AMÃ‰LIORER
print("\nğŸ’¡ Conseils pour amÃ©liorer :")
if test_accuracies[-1] < 95:
    print("   - Augmente le nombre d'Ã©poques (EPOCHS)")
    print("   - RÃ©duis le learning rate (LEARNING_RATE)")
    print("   - Augmente la taille du rÃ©seau")
elif test_accuracies[-1] < 98:
    print("   - Ajoute plus d'augmentation de donnÃ©es")
    print("   - Ajuste le dropout")
    print("   - Essaie diffÃ©rents optimiseurs")
else:
    print("   ğŸ‰ Excellent ! Ton IA est trÃ¨s performante !")

print(f"\nğŸ“Š Statistiques finales :")
print(f"   - AmÃ©lioration : {test_accuracies[-1] - test_accuracies[0]:.1f}%")
print(f"   - Ã‰cart train/test : {abs(train_accuracies[-1] - test_accuracies[-1]):.1f}%")
if abs(train_accuracies[-1] - test_accuracies[-1]) > 5:
    print("   âš ï¸ Possible surapprentissage - augmente le dropout")